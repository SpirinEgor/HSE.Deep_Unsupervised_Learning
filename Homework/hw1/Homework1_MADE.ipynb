{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Homework1_MADE.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SpirinEgor/HSE.Deep_Unsupervised_Learning/blob/hw1/Homework/hw1/Homework1_MADE.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4h-BuVcBGnnz"
   },
   "source": [
    "! nvidia-smi"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdy1FtrRpGcC"
   },
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wUVy2glDtoaR"
   },
   "source": [
    "! if [ -d HSE.Deep_Unsupervised_Learning ]; then rm -Rf HSE.Deep_Unsupervised_Learning; fi\n",
    "! git clone https://github.com/SpirinEgor/HSE.Deep_Unsupervised_Learning.git\n",
    "%cd HSE.Deep_Unsupervised_Learning\n",
    "! git checkout hw1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMHhUQ9KHW46"
   },
   "source": [
    "! pip install -r requirements.txt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cbGDjS-xHhvI"
   },
   "source": [
    "! git pull"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0Xdn8D-ahwcr"
   },
   "source": [
    "! unzip -qq Homework/hw1/data/hw1_data.zip -d Homework/hw1/data/\n",
    "! mv -v Homework/hw1/data/hw1_data/* Homework/hw1/data/\n",
    "! rm -rf Homework/hw1/data/hw1_data/"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZHWosWrbpO5Y"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from made.trainer import ImageMADETrainer\n",
    "from utils.hw1_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WPH-fXxLA3fG"
   },
   "source": [
    "plt.rcParams[\"axes.labelsize\"] = 25.0\n",
    "plt.rcParams[\"xtick.labelsize\"] = 20.0\n",
    "plt.rcParams[\"ytick.labelsize\"] = 20.0\n",
    "plt.rcParams[\"legend.fontsize\"] = 18.0\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [8.0, 6.0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk6l6G30tEIg"
   },
   "source": [
    "# MADE\n",
    "\n",
    "In this question, you will implement [MADE](https://arxiv.org/abs/1502.03509). In the first part, you will use MADE to model a simple 2D joint distribution, and in the second half, you will train MADE on image datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQMNxln-UxX"
   },
   "source": [
    "## Part (a) Fitting 2D Data\n",
    "\n",
    "First, you will work with bivariate data of the form $x = (x_0,x_1)$, where $x_0, x_1 \\in \\{0, \\dots, d\\}$. We can easily visualize a 2D dataset by plotting a 2D histogram. Run the cell below to visualize our datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "alF9C1t-tEys"
   },
   "source": [
    "visualize_q1a_data(dataset_type=1)\n",
    "visualize_q1a_data(dataset_type=2)\n",
    "# you can access data with get_data_q1_a(dset_type=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDywq5JZR4Eg"
   },
   "source": [
    "Implement and train a MADE model through maximum likelihood to represent $p(x_0, x_1)$ on the given datasets, with any autoregressive ordering of your choosing. \n",
    "\n",
    "A few notes:\n",
    "* You do not need to do training with multiple masks\n",
    "* You made find it useful to one-hot encode your inputs. \n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. Visualize the learned 2D distribution by plotting a 2D heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHHxHqSJTSAa"
   },
   "source": [
    "### Solution\n",
    "Fill out the function below and return the necessary arguments. Feel free to create more cells if need be."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wtFZU2ymB6_t"
   },
   "source": [
    "def q1_a(train_data, test_data, d, dset_id):\n",
    "    \"\"\"\n",
    "    train_data: An (n_train, 2) numpy array of integers in {0, ..., d-1}\n",
    "    test_data: An (n_test, 2) numpy array of integers in {0, .., d-1}\n",
    "    d: The number of possible discrete values for each random variable x1 and x2\n",
    "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "            used to set different hyperparameters for different datasets\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - a numpy array of size (d, d) of probabilities (the learned joint distribution)\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        1: {\n",
    "            \"hidden_layers\": [128, 128],\n",
    "            \"batch_size\": 10,\n",
    "            \"test_batch_size\": 20,\n",
    "            \"hyperparameters\": {\"lr\": 2e-3, \"n_epochs\": 20, \"weight_decay\": 1e-4, \"clip_norm\": 10},\n",
    "        },\n",
    "        2: {\n",
    "            \"hidden_layers\": [128, 128],\n",
    "            \"batch_size\": 10,\n",
    "            \"test_batch_size\": 20,\n",
    "            \"hyperparameters\": {\"lr\": 2e-3, \"n_epochs\": 20, \"weight_decay\": 1e-4, \"clip_norm\": 10},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    made_trainer = ImageMADETrainer((2,), d, config[dset_id][\"hidden_layers\"], True)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=config[dset_id][\"batch_size\"], shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=config[dset_id][\"test_batch_size\"])\n",
    "\n",
    "    train_losses, test_losses = made_trainer.train(\n",
    "        train_dataloader, seed=7, test_dataloader=test_dataloader, **config[dset_id][\"hyperparameters\"]\n",
    "    )\n",
    "\n",
    "    return train_losses, test_losses, made_trainer.made.get_distribution()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iso12bj4Tup8"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q2_a`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "njfGrZ74Jm2d"
   },
   "source": [
    "q1_save_results(1, \"a\", q1_a)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o6-NJJnIJoX6"
   },
   "source": [
    "q1_save_results(2, \"a\", q1_a)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4HezJRtW7H2"
   },
   "source": [
    "## Part (b) Shapes and MNIST\n",
    "Now, we will work with a higher dimensional datasets, namely a shape dataset and MNIST. Run the cell below to visualize the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F81U6yR1UUVq"
   },
   "source": [
    "visualize_q1b_data(1)\n",
    "visualize_q1b_data(2)\n",
    "# you can access data with get_data_q1_b(dset_type=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrJPvrYhUZYO"
   },
   "source": [
    "Implement and train a MADE model on the given binary image datasets. Given some binary image of height $H$ and width $W$, we can represent image $x\\in \\{0, 1\\}^{H\\times W}$ as a flattened binary vector $x\\in \\{0, 1\\}^{HW}$ to input into MADE to model $p_\\theta(x) = \\prod_{i=1}^{HW} p_\\theta(x_i|x_{<i})$. Your model should output logits, after which you could apply a sigmoid over 1 logit, or a softmax over two logits (either is fine).\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEgF3FCXV8zb"
   },
   "source": [
    "### Solution\n",
    "Fill out the function below and return the necessary arguments. Feel free to create more cells if need be."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WHLjAxjRVx_S"
   },
   "source": [
    "def q1_b(train_data, test_data, image_shape, dset_id, n_samples: int = 100):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    test_data: An (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    image_shape: (H, W), height and width of the image\n",
    "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "            used to set different hyperparameters for different datasets\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - a numpy array of size (100, H, W, 1) of samples with values in {0, 1}\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        1: {\n",
    "            \"hidden_layers\": [512, 512],\n",
    "            \"batch_size\": 10,\n",
    "            \"test_batch_size\": 20,\n",
    "            \"hyperparameters\": {\"lr\": 1e-3, \"n_epochs\": 20, \"weight_decay\": 1e-4, \"clip_norm\": 10},\n",
    "        },\n",
    "        2: {\n",
    "            \"hidden_layers\": [512, 512],\n",
    "            \"batch_size\": 10,\n",
    "            \"test_batch_size\": 20,\n",
    "            \"hyperparameters\": {\"lr\": 1e-3, \"n_epochs\": 20, \"weight_decay\": 1e-4, \"clip_norm\": 10},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    train_data = np.transpose(train_data, (0, 3, 1, 2))\n",
    "    test_data = np.transpose(test_data, (0, 3, 1, 2))\n",
    "    H, W = image_shape\n",
    "\n",
    "    made_trainer = ImageMADETrainer((1, H, W), 2, hidden_sizes=config[dset_id][\"hidden_layers\"])\n",
    "\n",
    "    train_dataloader = DataLoader(train_data.astype(int), batch_size=config[dset_id][\"batch_size\"], shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data.astype(int), batch_size=config[dset_id][\"test_batch_size\"])\n",
    "\n",
    "    train_losses, test_losses = made_trainer.train(\n",
    "        train_dataloader, seed=7, test_dataloader=test_dataloader, **config[dset_id][\"hyperparameters\"]\n",
    "    )\n",
    "\n",
    "    samples = made_trainer.made.sample(n_samples, (1, H, W))\n",
    "    samples = np.transpose(samples, (0, 2, 3, 1))\n",
    "\n",
    "    return train_losses, test_losses, samples"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBfCSKFnV-Mz"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q2_b`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "frAYhilEwG2x"
   },
   "source": [
    "q1_save_results(1, \"b\", q1_b)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B5nBFeI7wJeN"
   },
   "source": [
    "q1_save_results(2, \"b\", q1_b)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}